{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fb9685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2>Agentic RAG App Doc-store (Vector-store) and Embeddings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891872f1",
   "metadata": {},
   "source": [
    "Will get a doc/vector store established, and play around with it to see how it works with the groq LLM. Any missing information will then be filled by implementing with tavily (internet-search). Will then establish agents with specific roles with CrewAI. Write functions for this (week 1), and then establish the streamlit front-end (week 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739319b1",
   "metadata": {},
   "source": [
    "<h3>Doc-store (Vector-store) and Embeddings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0a1e0",
   "metadata": {},
   "source": [
    "1. Your documents live locally in your project\n",
    "\n",
    "2. You turn those documents into embeddings (numbers)\n",
    "\n",
    "3. You store those numbers locally in FAISS\n",
    "\n",
    "4. A user asks a question. The question is directed to the 'researcher' agent of CrewAI.\n",
    "\n",
    "5. The researcher finds the most relevant document chunks using FAISS, or use 'tavily' to do an internet search for the information.\n",
    "\n",
    "6. You send those chunks to Groq. The other agents use grow to fulfill their role.\n",
    "\n",
    "7. Groq writes the answer\n",
    "\n",
    "Groq never sees embeddings.\n",
    "FAISS never talks to Groq.\n",
    "The agents of CrewAI are agents, they use groq in different ways to fulfill their role e.g find an answer, write and answer, critique an answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba59ab",
   "metadata": {},
   "source": [
    "<h4>Creating the doc store (done using FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b1a9",
   "metadata": {},
   "source": [
    "We need to create a vector-store that can take in docuements and store them. We need to be able to save the store so we don't have to keep feeding it the same doceuments between uses. We also wnat to be able to add new docuemnts as we find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ce3332",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_offline_mode' from 'huggingface_hub' (/Users/cameroncochrane/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     TextLoader,\n\u001b[32m      9\u001b[39m     PyPDFLoader,\n\u001b[32m     10\u001b[39m     UnstructuredMarkdownLoader,\n\u001b[32m     11\u001b[39m     DirectoryLoader,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/sentence_transformers/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/sentence_transformers/backend/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/sentence_transformers/backend/load.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[32m     11\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/transformers/__init__.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     33\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     is_pretty_midi_available,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py:76\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     37\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     38\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     replace_return_docstrings,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     ContextManagers,\n\u001b[32m     46\u001b[39m     ExplicitEnum,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     transpose,\n\u001b[32m     75\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     77\u001b[39m     CHAT_TEMPLATE_DIR,\n\u001b[32m     78\u001b[39m     CHAT_TEMPLATE_FILE,\n\u001b[32m     79\u001b[39m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[32m     80\u001b[39m     HF_MODULES_CACHE,\n\u001b[32m     81\u001b[39m     LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n\u001b[32m     82\u001b[39m     S3_BUCKET_PREFIX,\n\u001b[32m     83\u001b[39m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[32m     84\u001b[39m     EntryNotFoundError,\n\u001b[32m     85\u001b[39m     PushInProgress,\n\u001b[32m     86\u001b[39m     PushToHubMixin,\n\u001b[32m     87\u001b[39m     RepositoryNotFoundError,\n\u001b[32m     88\u001b[39m     RevisionNotFoundError,\n\u001b[32m     89\u001b[39m     cached_file,\n\u001b[32m     90\u001b[39m     define_sagemaker_information,\n\u001b[32m     91\u001b[39m     extract_commit_hash,\n\u001b[32m     92\u001b[39m     has_file,\n\u001b[32m     93\u001b[39m     http_user_agent,\n\u001b[32m     94\u001b[39m     list_repo_templates,\n\u001b[32m     95\u001b[39m     try_to_load_from_cache,\n\u001b[32m     96\u001b[39m )\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     98\u001b[39m     ACCELERATE_MIN_VERSION,\n\u001b[32m     99\u001b[39m     BITSANDBYTES_MIN_VERSION,\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m     torch_only_method,\n\u001b[32m    249\u001b[39m )\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernel_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KernelConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpx\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     _CACHED_NO_EXIST,\n\u001b[32m     31\u001b[39m     CommitOperationAdd,\n\u001b[32m     32\u001b[39m     ModelCard,\n\u001b[32m     33\u001b[39m     ModelCardData,\n\u001b[32m     34\u001b[39m     constants,\n\u001b[32m     35\u001b[39m     create_branch,\n\u001b[32m     36\u001b[39m     create_commit,\n\u001b[32m     37\u001b[39m     create_repo,\n\u001b[32m     38\u001b[39m     hf_hub_download,\n\u001b[32m     39\u001b[39m     hf_hub_url,\n\u001b[32m     40\u001b[39m     is_offline_mode,\n\u001b[32m     41\u001b[39m     list_repo_tree,\n\u001b[32m     42\u001b[39m     snapshot_download,\n\u001b[32m     43\u001b[39m     try_to_load_from_cache,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m REGEX_COMMIT_HASH\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     47\u001b[39m     EntryNotFoundError,\n\u001b[32m     48\u001b[39m     GatedRepoError,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     hf_raise_for_status,\n\u001b[32m     57\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'is_offline_mode' from 'huggingface_hub' (/Users/cameroncochrane/Data Projects/Agentic-RAG/.venv/lib/python3.12/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "import langchain_core\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    DirectoryLoader,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a2e7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Scan a given directory and return the paths of all files inside.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to scan.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f25a4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Optional\n",
    "from __future__ import annotations\n",
    "import hashlib\n",
    "\n",
    "# wrapper around sentence_transformers to match LangChain embeddings interface\n",
    "class SentenceTransformerEmbeddingsWrapper:\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\", lazy: bool = True):\n",
    "        # lazy=True avoids downloading the model at import/definition time.\n",
    "        self.model_name = model_name\n",
    "        self._model: Optional[SentenceTransformer] = None\n",
    "        self.lazy = lazy\n",
    "        if not self.lazy:\n",
    "            self._ensure_model()\n",
    "\n",
    "    def _ensure_model(self) -> None:\n",
    "        if self._model is None:\n",
    "            self._model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Return embeddings for a list of documents.\n",
    "        This matches LangChain's `embed_documents` contract.\n",
    "        \"\"\"\n",
    "        self._ensure_model()\n",
    "        emb = self._model.encode(texts, show_progress_bar=False)\n",
    "        return emb.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        self._ensure_model()\n",
    "        emb = self._model.encode([text], show_progress_bar=False)\n",
    "        return emb[0].tolist()\n",
    "\n",
    "    def __call__(self, texts: Union[str, List[str]]):\n",
    "        # Support both single-query strings and lists of documents.\n",
    "        if isinstance(texts, str):\n",
    "            return self.embed_query(texts)\n",
    "        if isinstance(texts, (list, tuple)):\n",
    "            return self.embed_documents(list(texts))\n",
    "        raise TypeError(f\"Unsupported input type for embeddings: {type(texts)}\")\n",
    "    \n",
    "def create_faiss_store_from_documents(documents: List[Document], index_dir: str = \"vectorstore\", embedding_model: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Build a FAISS vectorstore from a list of langchain Document objects and save it to disk.\n",
    "    Returns the in-memory FAISS store.\n",
    "    \"\"\"\n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    # Pass the embeddings wrapper object so FAISS can access its\n",
    "    # `embed_documents` / `embed_query` methods as expected.\n",
    "    store = FAISS.from_documents(documents, embeddings)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "def load_faiss_store(index_dir: str = \"vectorstore\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load a previously saved FAISS vectorstore from disk.\n",
    "    \"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    # FAISS.load_local expects an embeddings object providing `embed_documents`\n",
    "    return FAISS.load_local(index_dir, embeddings)\n",
    "\n",
    "def add_documents_and_save(store: FAISS, new_documents: List[Document], index_dir: str = \"vectorstore\"):\n",
    "    \"\"\"\n",
    "    Add documents to an existing FAISS store and persist to disk.\n",
    "    \"\"\"\n",
    "    store.add_documents(new_documents)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "\n",
    "def path_upload_document_to_vectorstore(\n",
    "    document_paths: Union[str, List[str]],\n",
    "    store: FAISS,\n",
    "    index_dir: str = \"vectorstore\",\n",
    "    dedup_mode: str = \"content\",  # \"content\" (recommended) or \"source\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload one or more documents to an existing FAISS vectorstore and persist to disk\n",
    "    Ensures each Document has a `content_hash` in metadata so subsequent\n",
    "    content-based deduplication works even for the initial ingestion.\n",
    "    WITHOUT erasing existing contents, with deduplication.\n",
    "\n",
    "    # Ensure documents have a content_hash for idempotent ingestion\n",
    "    def _content_hash(text: str) -> str:\n",
    "        return hashlib.sha256((text or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    for d in documents:\n",
    "        # only operate on Document instances\n",
    "        if not isinstance(d, Document):\n",
    "            continue\n",
    "        h = d.metadata.get(\"content_hash\")\n",
    "        if not h:\n",
    "            h = _content_hash(d.page_content)\n",
    "            d.metadata[\"content_hash\"] = h\n",
    "        # ensure a source field exists for traceability\n",
    "        d.metadata.setdefault(\"source\", d.metadata.get(\"source\", \"\"))\n",
    "    Deduplication behavior:\n",
    "      - dedup_mode=\"content\": hashes each loaded Document.page_content and skips exact duplicates.\n",
    "      - dedup_mode=\"source\": skips if an existing Document has the same metadata[\"source\"].\n",
    "\n",
    "    Notes:\n",
    "      - For PDFs, PyPDFLoader returns one Document per page; dedup will happen at page level.\n",
    "      - This function assumes your loaders populate metadata[\"source\"] (LangChain usually does).\n",
    "    \"\"\"\n",
    "    if isinstance(document_paths, str):\n",
    "        document_paths = [document_paths]\n",
    "    \n",
    "    def _content_hash(text: str) -> str:\n",
    "        # stable content-based id\n",
    "        return hashlib.sha256((text or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "    def _get_existing_hashes(store: FAISS) -> set[str]:\n",
    "        \"\"\"\n",
    "        Extract content hashes from the existing LangChain FAISS docstore.\n",
    "        We store hashes in Document.metadata[\"content_hash\"] for idempotent ingestion.\n",
    "        \"\"\"\n",
    "        existing = set()\n",
    "\n",
    "        # LangChain FAISS keeps docs in an InMemoryDocstore at store.docstore._dict\n",
    "        doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None)\n",
    "        if isinstance(doc_dict, dict):\n",
    "            for d in doc_dict.values():\n",
    "                if isinstance(d, Document):\n",
    "                    h = d.metadata.get(\"content_hash\")\n",
    "                    if h:\n",
    "                        existing.add(h)\n",
    "        return existing\n",
    "\n",
    "    # Build the dedup index from the current store\n",
    "    existing_hashes = _get_existing_hashes(store) if dedup_mode == \"content\" else set()\n",
    "\n",
    "    existing_sources = set()\n",
    "    if dedup_mode == \"source\":\n",
    "        doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None)\n",
    "        if isinstance(doc_dict, dict):\n",
    "            for d in doc_dict.values():\n",
    "                if isinstance(d, Document):\n",
    "                    src = d.metadata.get(\"source\")\n",
    "                    if src:\n",
    "                        existing_sources.add(src)\n",
    "\n",
    "    total_added = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    for document_path in document_paths:\n",
    "        # Determine the loader type based on file extension\n",
    "        if document_path.lower().endswith(\".pdf\"):\n",
    "            loader_cls = PyPDFLoader\n",
    "        elif document_path.lower().endswith(\".txt\"):\n",
    "            loader_cls = TextLoader\n",
    "        elif document_path.lower().endswith(\".md\"):\n",
    "            loader_cls = UnstructuredMarkdownLoader\n",
    "        else:\n",
    "            print(f\"Unsupported file type for {document_path}. Supported types are: .pdf, .txt, .md\")\n",
    "            continue\n",
    "\n",
    "        # Load the document(s)\n",
    "        try:\n",
    "            loader = loader_cls(document_path)\n",
    "            loaded_docs = loader.load()\n",
    "            print(f\"Loaded {len(loaded_docs)} document(s) from {document_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document {document_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Tag documents with dedup metadata and filter duplicates\n",
    "        new_docs: List[Document] = []\n",
    "        for d in loaded_docs:\n",
    "            # Ensure source is set for traceability (helps source-based dedup & citations)\n",
    "            d.metadata.setdefault(\"source\", document_path)\n",
    "\n",
    "            if dedup_mode == \"source\":\n",
    "                src = d.metadata.get(\"source\")\n",
    "                if src in existing_sources:\n",
    "                    total_skipped += 1\n",
    "                    continue\n",
    "                existing_sources.add(src)\n",
    "                new_docs.append(d)\n",
    "                continue\n",
    "\n",
    "            # content-based dedup (recommended)\n",
    "            h = d.metadata.get(\"content_hash\")\n",
    "            if not h:\n",
    "                h = _content_hash(d.page_content)\n",
    "                d.metadata[\"content_hash\"] = h\n",
    "\n",
    "            if h in existing_hashes:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "\n",
    "            existing_hashes.add(h)\n",
    "            new_docs.append(d)\n",
    "\n",
    "        if not new_docs:\n",
    "            print(f\"No new chunks/pages to add from {document_path} (all duplicates).\")\n",
    "            continue\n",
    "\n",
    "        # Add to store and persist\n",
    "        store.add_documents(new_docs)\n",
    "        store.save_local(index_dir)\n",
    "        total_added += len(new_docs)\n",
    "\n",
    "        print(f\"Added {len(new_docs)} new document(s) from {document_path} and saved to '{index_dir}'.\")\n",
    "\n",
    "    print(f\"Done. Added: {total_added}, skipped as duplicates: {total_skipped}.\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a84bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2507.41it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-small-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# load the initial document (init_document.pdf)\n",
    "run = False\n",
    "if run == True:\n",
    "    initial_document_directory = \"initial_document\" # When first creating the store, use a test document. Next, when adding more documents, use the path_upload_document_to_vectorstore() function with the main documents in 'documents' folder\n",
    "    loader = DirectoryLoader(initial_document_directory)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Create a FAISS vector store from the loaded documents\n",
    "    store = create_faiss_store_from_documents(docs, index_dir=\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d67ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "Added 15 new document(s) from documents/c1cs15013h.pdf and saved to 'vectorstore'.\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "Added 13 new document(s) from documents/c5cs00105f.pdf and saved to 'vectorstore'.\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "Added 65 new document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "Added 17 new document(s) from documents/c9np00039a.pdf and saved to 'vectorstore'.\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "Added 42 new document(s) from documents/c5ob00169b.pdf and saved to 'vectorstore'.\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "Added 7 new document(s) from documents/d2qo00043a.pdf and saved to 'vectorstore'.\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "Added 48 new document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "Added 19 new document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf and saved to 'vectorstore'.\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "Added 15 new document(s) from documents/c3np70090a.pdf and saved to 'vectorstore'.\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "Added 24 new document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "Added 19 new document(s) from documents/c5np00046g.pdf and saved to 'vectorstore'.\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "Added 12 new document(s) from documents/b901245c.pdf and saved to 'vectorstore'.\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "Added 34 new document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "Added 43 new document(s) from documents/c8cs00716k.pdf and saved to 'vectorstore'.\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "Added 39 new document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "Added 50 new document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "Added 19 new document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "Added 29 new document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf and saved to 'vectorstore'.\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "Added 11 new document(s) from documents/c3qo00086a.pdf and saved to 'vectorstore'.\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "Added 16 new document(s) from documents/c8cs00379c.pdf and saved to 'vectorstore'.\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "Added 11 new document(s) from documents/b816703f.pdf and saved to 'vectorstore'.\n",
      "Done. Added: 548, skipped as duplicates: 0.\n"
     ]
    }
   ],
   "source": [
    "# Update FAISS store with all files under `directory_path` (documents/) (These are the main and bulk of the vestorstore)\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "store = path_upload_document_to_vectorstore(file_paths, store, index_dir=\"vectorstore\", dedup_mode=\"content\")\n",
    "# Note: this notebook now computes and persists metadata['content_hash'] at initial ingestion.\n",
    "# New indexes created after this change will include content hashes and enable immediate\n",
    "# content-based deduplication. If you have an older index created before this change,\n",
    "# rebuild it or run the upload path to add hashes before relying on content deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "No new chunks/pages to add from documents/c1cs15013h.pdf (all duplicates).\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "No new chunks/pages to add from documents/c5cs00105f.pdf (all duplicates).\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "No new chunks/pages to add from documents/c9np00039a.pdf (all duplicates).\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "No new chunks/pages to add from documents/c5ob00169b.pdf (all duplicates).\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "No new chunks/pages to add from documents/d2qo00043a.pdf (all duplicates).\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "No new chunks/pages to add from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf (all duplicates).\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "No new chunks/pages to add from documents/c3np70090a.pdf (all duplicates).\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "No new chunks/pages to add from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "No new chunks/pages to add from documents/c5np00046g.pdf (all duplicates).\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "No new chunks/pages to add from documents/b901245c.pdf (all duplicates).\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00716k.pdf (all duplicates).\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/radical-reactions-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "No new chunks/pages to add from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "No new chunks/pages to add from documents/c3qo00086a.pdf (all duplicates).\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00379c.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "No new chunks/pages to add from documents/b816703f.pdf (all duplicates).\n",
      "Done. Added: 0, skipped as duplicates: 548.\n"
     ]
    }
   ],
   "source": [
    "# Re-run to test it's duplicate spotting capabilities\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "store = path_upload_document_to_vectorstore(file_paths, store, index_dir=\"vectorstore\", dedup_mode=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd371906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working nicely!! It spots duplicates!\n",
    "\n",
    "# Can we import a docstore for use betweem sessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c2ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS store from 'vectorstore' with 549 document(s).\n"
     ]
    }
   ],
   "source": [
    "def load_docstore_from_dir(index_dir: str = \"vectorstore\", embedding_model: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Load a FAISS-backed docstore from disk and return (store, documents_list).\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(index_dir):\n",
    "        raise FileNotFoundError(f\"Index directory '{index_dir}' not found.\")\n",
    "\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    try:\n",
    "        store = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load FAISS store from '{index_dir}': {e}\")\n",
    "\n",
    "    doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None) or {}\n",
    "    docs = [d for d in doc_dict.values() if isinstance(d, Document)]\n",
    "\n",
    "    print(f\"Loaded FAISS store from '{index_dir}' with {len(docs)} document(s).\")\n",
    "    return store, docs\n",
    "\n",
    "loaded_vstore, laoded_vstore_docs = load_docstore_from_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88f8aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "No new chunks/pages to add from documents/c1cs15013h.pdf (all duplicates).\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "No new chunks/pages to add from documents/c5cs00105f.pdf (all duplicates).\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "No new chunks/pages to add from documents/c9np00039a.pdf (all duplicates).\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "No new chunks/pages to add from documents/c5ob00169b.pdf (all duplicates).\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "No new chunks/pages to add from documents/d2qo00043a.pdf (all duplicates).\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "No new chunks/pages to add from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf (all duplicates).\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "No new chunks/pages to add from documents/c3np70090a.pdf (all duplicates).\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "No new chunks/pages to add from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "No new chunks/pages to add from documents/c5np00046g.pdf (all duplicates).\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "No new chunks/pages to add from documents/b901245c.pdf (all duplicates).\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00716k.pdf (all duplicates).\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/radical-reactions-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "No new chunks/pages to add from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "No new chunks/pages to add from documents/c3qo00086a.pdf (all duplicates).\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00379c.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "No new chunks/pages to add from documents/b816703f.pdf (all duplicates).\n",
      "Done. Added: 0, skipped as duplicates: 548.\n"
     ]
    }
   ],
   "source": [
    "# Re-run to test it's duplicate spotting capabilities\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "# loaded_vstore is a (store, docs) tuple returned by load_docstore_from_dir()\n",
    "vstore = path_upload_document_to_vectorstore(file_paths, loaded_vstore, index_dir=\"vectorstore\", dedup_mode=\"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38cfe7",
   "metadata": {},
   "source": [
    "<h4>Querying the docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your store (must match the embedding model used to build it)\n",
    "store = load_faiss_store(\n",
    "    index_dir=\"vectorstore\",\n",
    "    embedding_model=\"BAAI/bge-small-en-v1.5\",\n",
    ")\n",
    "\n",
    "query = \"What is the main topic of the document?\"\n",
    "k = 5\n",
    "\n",
    "# Retrieve top-k chunks\n",
    "docs = store.similarity_search(query, k=k)\n",
    "\n",
    "print(f\"Query: {query}\\nTop {k} results:\")\n",
    "for i, d in enumerate(docs, start=1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Result {i}\")\n",
    "    print(\"Source:\", d.metadata.get(\"source\"))\n",
    "    print(\"Page:\", d.metadata.get(\"page\"))\n",
    "    print(\"Hash:\", d.metadata.get(\"content_hash\"))\n",
    "    print(\"-\"*80)\n",
    "    print(d.page_content[:800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
