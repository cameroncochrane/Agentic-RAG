{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fb9685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2>Agentic RAG App Doc-store (Vector-store) and Embeddings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891872f1",
   "metadata": {},
   "source": [
    "Will get a doc/vector store established, and play around with it to see how it works with the groq LLM. Any missing information will then be filled by implementing with tavily (internet-search). Will then establish agents with specific roles with CrewAI. Write functions for this (week 1), and then establish the streamlit front-end (week 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739319b1",
   "metadata": {},
   "source": [
    "<h3>Doc-store (Vector-store) and Embeddings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0a1e0",
   "metadata": {},
   "source": [
    "1. Your documents live locally in your project\n",
    "\n",
    "2. You turn those documents into embeddings (numbers)\n",
    "\n",
    "3. You store those numbers locally in FAISS\n",
    "\n",
    "4. A user asks a question. The question is directed to the 'researcher' agent of CrewAI.\n",
    "\n",
    "5. The researcher finds the most relevant document chunks using FAISS, or use 'tavily' to do an internet search for the information.\n",
    "\n",
    "6. You send those chunks to Groq. The other agents use grow to fulfill their role.\n",
    "\n",
    "7. Groq writes the answer\n",
    "\n",
    "Groq never sees embeddings.\n",
    "FAISS never talks to Groq.\n",
    "The agents of CrewAI are agents, they use groq in different ways to fulfill their role e.g find an answer, write and answer, critique an answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba59ab",
   "metadata": {},
   "source": [
    "<h4>Creating the doc store (done using FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b1a9",
   "metadata": {},
   "source": [
    "We need to create a vector-store that can take in docuements and store them. We need to be able to save the store so we don't have to keep feeding it the same doceuments between uses. We also wnat to be able to add new docuemnts as we find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ce3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    DirectoryLoader,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25a4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "# wrapper around sentence_transformers to match LangChain embeddings interface\n",
    "class SentenceTransformerEmbeddingsWrapper:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        # returns list[list[float]]\n",
    "        emb = self.model.encode(texts, show_progress_bar=False)\n",
    "        return emb.tolist()\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        emb = self.model.encode([text], show_progress_bar=False)\n",
    "        return emb[0].tolist()\n",
    "\n",
    "def create_faiss_store_from_documents(documents: List[Document], index_dir: str = \"faiss_index\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Build a FAISS vectorstore from a list of langchain Document objects and save it to disk.\n",
    "    Returns the in-memory FAISS store.\n",
    "    \"\"\"\n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    store = FAISS.from_documents(documents, embeddings)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "def load_faiss_store(index_dir: str = \"faiss_index\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load a previously saved FAISS vectorstore from disk.\n",
    "    \"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    return FAISS.load_local(index_dir, embeddings)\n",
    "\n",
    "def add_documents_and_save(store: FAISS, new_documents: List[Document], index_dir: str = \"faiss_index\"):\n",
    "    \"\"\"\n",
    "    Add documents to an existing FAISS store and persist to disk.\n",
    "    \"\"\"\n",
    "    store.add_documents(new_documents)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "# Example: load documents from a directory and create/save the index\n",
    "# loader = DirectoryLoader(\"path/to/docs\", loader_cls=TextLoader)\n",
    "# docs = loader.load()\n",
    "# store = create_faiss_store_from_documents(docs, index_dir=\"faiss_index\")\n",
    "# later you can reload with:\n",
    "# store = load_faiss_store(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedbb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
