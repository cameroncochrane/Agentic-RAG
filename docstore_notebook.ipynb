{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fb9685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2>Agentic RAG App Doc-store (Vector-store) and Embeddings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891872f1",
   "metadata": {},
   "source": [
    "Will get a doc/vector store established, and play around with it to see how it works with the groq LLM. Any missing information will then be filled by implementing with tavily (internet-search). Will then establish agents with specific roles with CrewAI. Write functions for this (week 1), and then establish the streamlit front-end (week 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739319b1",
   "metadata": {},
   "source": [
    "<h3>Doc-store (Vector-store) and Embeddings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0a1e0",
   "metadata": {},
   "source": [
    "1. Your documents live locally in your project\n",
    "\n",
    "2. You turn those documents into embeddings (numbers)\n",
    "\n",
    "3. You store those numbers locally in FAISS\n",
    "\n",
    "4. A user asks a question. The question is directed to the 'researcher' agent of CrewAI.\n",
    "\n",
    "5. The researcher finds the most relevant document chunks using FAISS, or use 'tavily' to do an internet search for the information.\n",
    "\n",
    "6. You send those chunks to Groq. The other agents use grow to fulfill their role.\n",
    "\n",
    "7. Groq writes the answer\n",
    "\n",
    "Groq never sees embeddings.\n",
    "FAISS never talks to Groq.\n",
    "The agents of CrewAI are agents, they use groq in different ways to fulfill their role e.g find an answer, write and answer, critique an answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba59ab",
   "metadata": {},
   "source": [
    "<h4>Creating the doc store (done using FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b1a9",
   "metadata": {},
   "source": [
    "We need to create a vector-store that can take in docuements and store them. We need to be able to save the store so we don't have to keep feeding it the same doceuments between uses. We also wnat to be able to add new docuemnts as we find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30ce3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    DirectoryLoader,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a2e7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Scan a given directory and return the paths of all files inside.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to scan.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f25a4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Optional\n",
    "from __future__ import annotations\n",
    "import hashlib\n",
    "\n",
    "# wrapper around sentence_transformers to match LangChain embeddings interface\n",
    "class SentenceTransformerEmbeddingsWrapper:\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\", lazy: bool = True):\n",
    "        # lazy=True avoids downloading the model at import/definition time.\n",
    "        self.model_name = model_name\n",
    "        self._model: Optional[SentenceTransformer] = None\n",
    "        self.lazy = lazy\n",
    "        if not self.lazy:\n",
    "            self._ensure_model()\n",
    "\n",
    "    def _ensure_model(self) -> None:\n",
    "        if self._model is None:\n",
    "            self._model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Return embeddings for a list of documents.\n",
    "        This matches LangChain's `embed_documents` contract.\n",
    "        \"\"\"\n",
    "        self._ensure_model()\n",
    "        emb = self._model.encode(texts, show_progress_bar=False)\n",
    "        return emb.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        self._ensure_model()\n",
    "        emb = self._model.encode([text], show_progress_bar=False)\n",
    "        return emb[0].tolist()\n",
    "\n",
    "    def __call__(self, texts: Union[str, List[str]]):\n",
    "        # Support both single-query strings and lists of documents.\n",
    "        if isinstance(texts, str):\n",
    "            return self.embed_query(texts)\n",
    "        if isinstance(texts, (list, tuple)):\n",
    "            return self.embed_documents(list(texts))\n",
    "        raise TypeError(f\"Unsupported input type for embeddings: {type(texts)}\")\n",
    "    \n",
    "def create_faiss_store_from_documents(documents: List[Document], index_dir: str = \"vectorstore\", embedding_model: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Build a FAISS vectorstore from a list of langchain Document objects and save it to disk.\n",
    "    Returns the in-memory FAISS store.\n",
    "    \"\"\"\n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    # Pass the embeddings wrapper object so FAISS can access its\n",
    "    # `embed_documents` / `embed_query` methods as expected.\n",
    "    store = FAISS.from_documents(documents, embeddings)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "def load_faiss_store(index_dir: str = \"vectorstore\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load a previously saved FAISS vectorstore from disk.\n",
    "    \"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    # FAISS.load_local expects an embeddings object providing `embed_documents`\n",
    "    return FAISS.load_local(index_dir, embeddings)\n",
    "\n",
    "def add_documents_and_save(store: FAISS, new_documents: List[Document], index_dir: str = \"vectorstore\"):\n",
    "    \"\"\"\n",
    "    Add documents to an existing FAISS store and persist to disk.\n",
    "    \"\"\"\n",
    "    store.add_documents(new_documents)\n",
    "    store.save_local(index_dir)\n",
    "    return store\n",
    "\n",
    "\n",
    "def path_upload_document_to_vectorstore(\n",
    "    document_paths: Union[str, List[str]],\n",
    "    store: FAISS,\n",
    "    index_dir: str = \"vectorstore\",\n",
    "    dedup_mode: str = \"content\",  # \"content\" (recommended) or \"source\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload one or more documents to an existing FAISS vectorstore and persist to disk\n",
    "    Ensures each Document has a `content_hash` in metadata so subsequent\n",
    "    content-based deduplication works even for the initial ingestion.\n",
    "    WITHOUT erasing existing contents, with deduplication.\n",
    "\n",
    "    # Ensure documents have a content_hash for idempotent ingestion\n",
    "    def _content_hash(text: str) -> str:\n",
    "        return hashlib.sha256((text or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    for d in documents:\n",
    "        # only operate on Document instances\n",
    "        if not isinstance(d, Document):\n",
    "            continue\n",
    "        h = d.metadata.get(\"content_hash\")\n",
    "        if not h:\n",
    "            h = _content_hash(d.page_content)\n",
    "            d.metadata[\"content_hash\"] = h\n",
    "        # ensure a source field exists for traceability\n",
    "        d.metadata.setdefault(\"source\", d.metadata.get(\"source\", \"\"))\n",
    "    Deduplication behavior:\n",
    "      - dedup_mode=\"content\": hashes each loaded Document.page_content and skips exact duplicates.\n",
    "      - dedup_mode=\"source\": skips if an existing Document has the same metadata[\"source\"].\n",
    "\n",
    "    Notes:\n",
    "      - For PDFs, PyPDFLoader returns one Document per page; dedup will happen at page level.\n",
    "      - This function assumes your loaders populate metadata[\"source\"] (LangChain usually does).\n",
    "    \"\"\"\n",
    "    if isinstance(document_paths, str):\n",
    "        document_paths = [document_paths]\n",
    "    \n",
    "    def _content_hash(text: str) -> str:\n",
    "        # stable content-based id\n",
    "        return hashlib.sha256((text or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "    def _get_existing_hashes(store: FAISS) -> set[str]:\n",
    "        \"\"\"\n",
    "        Extract content hashes from the existing LangChain FAISS docstore.\n",
    "        We store hashes in Document.metadata[\"content_hash\"] for idempotent ingestion.\n",
    "        \"\"\"\n",
    "        existing = set()\n",
    "\n",
    "        # LangChain FAISS keeps docs in an InMemoryDocstore at store.docstore._dict\n",
    "        doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None)\n",
    "        if isinstance(doc_dict, dict):\n",
    "            for d in doc_dict.values():\n",
    "                if isinstance(d, Document):\n",
    "                    h = d.metadata.get(\"content_hash\")\n",
    "                    if h:\n",
    "                        existing.add(h)\n",
    "        return existing\n",
    "\n",
    "    # Build the dedup index from the current store\n",
    "    existing_hashes = _get_existing_hashes(store) if dedup_mode == \"content\" else set()\n",
    "\n",
    "    existing_sources = set()\n",
    "    if dedup_mode == \"source\":\n",
    "        doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None)\n",
    "        if isinstance(doc_dict, dict):\n",
    "            for d in doc_dict.values():\n",
    "                if isinstance(d, Document):\n",
    "                    src = d.metadata.get(\"source\")\n",
    "                    if src:\n",
    "                        existing_sources.add(src)\n",
    "\n",
    "    total_added = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    for document_path in document_paths:\n",
    "        # Determine the loader type based on file extension\n",
    "        if document_path.lower().endswith(\".pdf\"):\n",
    "            loader_cls = PyPDFLoader\n",
    "        elif document_path.lower().endswith(\".txt\"):\n",
    "            loader_cls = TextLoader\n",
    "        elif document_path.lower().endswith(\".md\"):\n",
    "            loader_cls = UnstructuredMarkdownLoader\n",
    "        else:\n",
    "            print(f\"Unsupported file type for {document_path}. Supported types are: .pdf, .txt, .md\")\n",
    "            continue\n",
    "\n",
    "        # Load the document(s)\n",
    "        try:\n",
    "            loader = loader_cls(document_path)\n",
    "            loaded_docs = loader.load()\n",
    "            print(f\"Loaded {len(loaded_docs)} document(s) from {document_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document {document_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Tag documents with dedup metadata and filter duplicates\n",
    "        new_docs: List[Document] = []\n",
    "        for d in loaded_docs:\n",
    "            # Ensure source is set for traceability (helps source-based dedup & citations)\n",
    "            d.metadata.setdefault(\"source\", document_path)\n",
    "\n",
    "            if dedup_mode == \"source\":\n",
    "                src = d.metadata.get(\"source\")\n",
    "                if src in existing_sources:\n",
    "                    total_skipped += 1\n",
    "                    continue\n",
    "                existing_sources.add(src)\n",
    "                new_docs.append(d)\n",
    "                continue\n",
    "\n",
    "            # content-based dedup (recommended)\n",
    "            h = d.metadata.get(\"content_hash\")\n",
    "            if not h:\n",
    "                h = _content_hash(d.page_content)\n",
    "                d.metadata[\"content_hash\"] = h\n",
    "\n",
    "            if h in existing_hashes:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "\n",
    "            existing_hashes.add(h)\n",
    "            new_docs.append(d)\n",
    "\n",
    "        if not new_docs:\n",
    "            print(f\"No new chunks/pages to add from {document_path} (all duplicates).\")\n",
    "            continue\n",
    "\n",
    "        # Add to store and persist\n",
    "        store.add_documents(new_docs)\n",
    "        store.save_local(index_dir)\n",
    "        total_added += len(new_docs)\n",
    "\n",
    "        print(f\"Added {len(new_docs)} new document(s) from {document_path} and saved to '{index_dir}'.\")\n",
    "\n",
    "    print(f\"Done. Added: {total_added}, skipped as duplicates: {total_skipped}.\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1a84bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2507.41it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-small-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# load the initial document (init_document.pdf)\n",
    "initial_document_directory = \"initial_document\" # When first creating the store, use a test document. Next, when adding more documents, use the path_upload_document_to_vectorstore() function with the main documents in 'documents' folder\n",
    "loader = DirectoryLoader(initial_document_directory)\n",
    "docs = loader.load()\n",
    "\n",
    "# Create a FAISS vector store from the loaded documents\n",
    "store = create_faiss_store_from_documents(docs, index_dir=\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d67ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "Added 15 new document(s) from documents/c1cs15013h.pdf and saved to 'vectorstore'.\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "Added 13 new document(s) from documents/c5cs00105f.pdf and saved to 'vectorstore'.\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "Added 65 new document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "Added 17 new document(s) from documents/c9np00039a.pdf and saved to 'vectorstore'.\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "Added 42 new document(s) from documents/c5ob00169b.pdf and saved to 'vectorstore'.\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "Added 7 new document(s) from documents/d2qo00043a.pdf and saved to 'vectorstore'.\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "Added 48 new document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "Added 19 new document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf and saved to 'vectorstore'.\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "Added 15 new document(s) from documents/c3np70090a.pdf and saved to 'vectorstore'.\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "Added 24 new document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "Added 19 new document(s) from documents/c5np00046g.pdf and saved to 'vectorstore'.\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "Added 12 new document(s) from documents/b901245c.pdf and saved to 'vectorstore'.\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "Added 34 new document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "Added 43 new document(s) from documents/c8cs00716k.pdf and saved to 'vectorstore'.\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "Added 39 new document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "Added 50 new document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "Added 19 new document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf and saved to 'vectorstore'.\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "Added 29 new document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf and saved to 'vectorstore'.\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "Added 11 new document(s) from documents/c3qo00086a.pdf and saved to 'vectorstore'.\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "Added 16 new document(s) from documents/c8cs00379c.pdf and saved to 'vectorstore'.\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "Added 11 new document(s) from documents/b816703f.pdf and saved to 'vectorstore'.\n",
      "Done. Added: 548, skipped as duplicates: 0.\n"
     ]
    }
   ],
   "source": [
    "# Update FAISS store with all files under `directory_path` (documents/) (These are the main and bulk of the vestorstore)\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "store = path_upload_document_to_vectorstore(file_paths, store, index_dir=\"vectorstore\", dedup_mode=\"content\")\n",
    "# Note: this notebook now computes and persists metadata['content_hash'] at initial ingestion.\n",
    "# New indexes created after this change will include content hashes and enable immediate\n",
    "# content-based deduplication. If you have an older index created before this change,\n",
    "# rebuild it or run the upload path to add hashes before relying on content deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "No new chunks/pages to add from documents/c1cs15013h.pdf (all duplicates).\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "No new chunks/pages to add from documents/c5cs00105f.pdf (all duplicates).\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "No new chunks/pages to add from documents/c9np00039a.pdf (all duplicates).\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "No new chunks/pages to add from documents/c5ob00169b.pdf (all duplicates).\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "No new chunks/pages to add from documents/d2qo00043a.pdf (all duplicates).\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "No new chunks/pages to add from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf (all duplicates).\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "No new chunks/pages to add from documents/c3np70090a.pdf (all duplicates).\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "No new chunks/pages to add from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "No new chunks/pages to add from documents/c5np00046g.pdf (all duplicates).\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "No new chunks/pages to add from documents/b901245c.pdf (all duplicates).\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00716k.pdf (all duplicates).\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/radical-reactions-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "No new chunks/pages to add from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "No new chunks/pages to add from documents/c3qo00086a.pdf (all duplicates).\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00379c.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "No new chunks/pages to add from documents/b816703f.pdf (all duplicates).\n",
      "Done. Added: 0, skipped as duplicates: 548.\n"
     ]
    }
   ],
   "source": [
    "# Re-run to test it's duplicate spotting capabilities\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "store = path_upload_document_to_vectorstore(file_paths, store, index_dir=\"vectorstore\", dedup_mode=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd371906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working nicely!! It spots duplicates!\n",
    "\n",
    "# Can we import a docstore for use betweem sessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c2ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS store from 'vectorstore' with 549 document(s).\n"
     ]
    }
   ],
   "source": [
    "def load_docstore_from_dir(index_dir: str = \"vectorstore\", embedding_model: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Load a FAISS-backed docstore from disk and return (store, documents_list).\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(index_dir):\n",
    "        raise FileNotFoundError(f\"Index directory '{index_dir}' not found.\")\n",
    "\n",
    "    embeddings = SentenceTransformerEmbeddingsWrapper(embedding_model)\n",
    "    try:\n",
    "        store = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load FAISS store from '{index_dir}': {e}\")\n",
    "\n",
    "    doc_dict = getattr(getattr(store, \"docstore\", None), \"_dict\", None) or {}\n",
    "    docs = [d for d in doc_dict.values() if isinstance(d, Document)]\n",
    "\n",
    "    print(f\"Loaded FAISS store from '{index_dir}' with {len(docs)} document(s).\")\n",
    "    return store, docs\n",
    "\n",
    "loaded_vstore, laoded_vstore_docs = load_docstore_from_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88f8aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document(s) from documents/c1cs15013h.pdf.\n",
      "No new chunks/pages to add from documents/c1cs15013h.pdf (all duplicates).\n",
      "Loaded 13 document(s) from documents/c5cs00105f.pdf.\n",
      "No new chunks/pages to add from documents/c5cs00105f.pdf (all duplicates).\n",
      "Loaded 65 document(s) from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/photochemical-approaches-to-complex-chemotypes-applications-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 17 document(s) from documents/c9np00039a.pdf.\n",
      "No new chunks/pages to add from documents/c9np00039a.pdf (all duplicates).\n",
      "Loaded 42 document(s) from documents/c5ob00169b.pdf.\n",
      "No new chunks/pages to add from documents/c5ob00169b.pdf (all duplicates).\n",
      "Loaded 7 document(s) from documents/d2qo00043a.pdf.\n",
      "No new chunks/pages to add from documents/d2qo00043a.pdf (all duplicates).\n",
      "Loaded 48 document(s) from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf.\n",
      "No new chunks/pages to add from documents/natural-product-synthesis-using-multicomponent-reaction-strategies.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2009 - Kumar - Synthesis of Natural Product Inspired Compound Collections.pdf (all duplicates).\n",
      "Loaded 15 document(s) from documents/c3np70090a.pdf.\n",
      "No new chunks/pages to add from documents/c3np70090a.pdf (all duplicates).\n",
      "Loaded 24 document(s) from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf.\n",
      "No new chunks/pages to add from documents/Recent progress in the total synthesis of pyrrolecontaining natural products.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/c5np00046g.pdf.\n",
      "No new chunks/pages to add from documents/c5np00046g.pdf (all duplicates).\n",
      "Loaded 12 document(s) from documents/b901245c.pdf.\n",
      "No new chunks/pages to add from documents/b901245c.pdf (all duplicates).\n",
      "Loaded 34 document(s) from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/semipinacol-rearrangement-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 43 document(s) from documents/c8cs00716k.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00716k.pdf (all duplicates).\n",
      "Loaded 39 document(s) from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Angew Chem Int Ed - 2016 - Chung - Stereoselective Halogenation in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 50 document(s) from documents/radical-reactions-in-natural-product-synthesis.pdf.\n",
      "No new chunks/pages to add from documents/radical-reactions-in-natural-product-synthesis.pdf (all duplicates).\n",
      "Loaded 19 document(s) from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf.\n",
      "No new chunks/pages to add from documents/Liebigs Annalen - June 23  1997 - Nicolaou - The Wittig and Related Reactions in Natural Product Synthesis.pdf (all duplicates).\n",
      "Loaded 29 document(s) from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf.\n",
      "No new chunks/pages to add from documents/recent-advances-in-natural-product-synthesis-by-using-intramolecular-diels-alder-reactions.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/c3qo00086a.pdf.\n",
      "No new chunks/pages to add from documents/c3qo00086a.pdf (all duplicates).\n",
      "Loaded 16 document(s) from documents/c8cs00379c.pdf.\n",
      "No new chunks/pages to add from documents/c8cs00379c.pdf (all duplicates).\n",
      "Loaded 11 document(s) from documents/b816703f.pdf.\n",
      "No new chunks/pages to add from documents/b816703f.pdf (all duplicates).\n",
      "Done. Added: 0, skipped as duplicates: 548.\n"
     ]
    }
   ],
   "source": [
    "# Re-run to test it's duplicate spotting capabilities\n",
    "main_documents_directory = \"documents\"\n",
    "file_paths = scan_directory(main_documents_directory)\n",
    "# loaded_vstore is a (store, docs) tuple returned by load_docstore_from_dir()\n",
    "vstore = path_upload_document_to_vectorstore(file_paths, loaded_vstore, index_dir=\"vectorstore\", dedup_mode=\"content\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
